{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fea5f46",
   "metadata": {},
   "source": [
    "# Transfer Learning Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0902182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "    \n",
    "import os, json, sys, time, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from  easydict import EasyDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from steves_models.steves_ptn import Steves_Prototypical_Network\n",
    "\n",
    "from steves_utils.lazy_iterable_wrapper import Lazy_Iterable_Wrapper\n",
    "from steves_utils.iterable_aggregator import Iterable_Aggregator\n",
    "from steves_utils.ptn_train_eval_test_jig import  PTN_Train_Eval_Test_Jig\n",
    "from steves_utils.torch_sequential_builder import build_sequential\n",
    "from steves_utils.torch_utils import get_dataset_metrics, ptn_confusion_by_domain_over_dataloader\n",
    "from steves_utils.utils_v2 import (per_domain_accuracy_from_confusion, get_datasets_base_path)\n",
    "from steves_utils.PTN.utils import independent_accuracy_assesment\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from steves_utils.stratified_dataset.episodic_accessor import Episodic_Accessor_Factory\n",
    "\n",
    "from steves_utils.ptn_do_report import (\n",
    "    get_loss_curve,\n",
    "    get_results_table,\n",
    "    get_parameters_table,\n",
    "    get_domain_accuracies,\n",
    ")\n",
    "\n",
    "from steves_utils.transforms import get_chained_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c840b4",
   "metadata": {},
   "source": [
    "# Allowed Parameters\n",
    "These are allowed parameters, not defaults\n",
    "Each of these values need to be present in the injected parameters (the notebook will raise an exception if they are not present)\n",
    "\n",
    "Papermill uses the cell tag \"parameters\" to inject the real parameters below this cell.\n",
    "Enable tags to see what I mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd44eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_parameters = {\n",
    "    \"experiment_name\",\n",
    "    \"lr\",\n",
    "    \"device\",\n",
    "    \"seed\",\n",
    "    \"dataset_seed\",\n",
    "    \"n_shot\",\n",
    "    \"n_query\",\n",
    "    \"n_way\",\n",
    "    \"train_k_factor\",\n",
    "    \"val_k_factor\",\n",
    "    \"test_k_factor\",\n",
    "    \"n_epoch\",\n",
    "    \"patience\",\n",
    "    \"criteria_for_best\",\n",
    "    \"x_net\",\n",
    "    \"datasets\",\n",
    "    \"torch_default_dtype\",\n",
    "    \"NUM_LOGS_PER_EPOCH\",\n",
    "    \"BEST_MODEL_PATH\",\n",
    "    \"x_shape\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa3f0049",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "from steves_utils.CORES.utils import (\n",
    "    ALL_NODES,\n",
    "    ALL_NODES_MINIMUM_1000_EXAMPLES,\n",
    "    ALL_DAYS\n",
    ")\n",
    "\n",
    "from steves_utils.ORACLE.utils_v2 import (\n",
    "    ALL_DISTANCES_FEET_NARROWED,\n",
    "    ALL_RUNS,\n",
    "    ALL_SERIAL_NUMBERS,\n",
    ")\n",
    "\n",
    "standalone_parameters = {}\n",
    "standalone_parameters[\"experiment_name\"] = \"STANDALONE PTN\"\n",
    "standalone_parameters[\"lr\"] = 0.001\n",
    "standalone_parameters[\"device\"] = \"cuda\"\n",
    "\n",
    "standalone_parameters[\"seed\"] = 1337\n",
    "standalone_parameters[\"dataset_seed\"] = 1337\n",
    "\n",
    "standalone_parameters[\"n_way\"] = 8\n",
    "standalone_parameters[\"n_shot\"] = 3\n",
    "standalone_parameters[\"n_query\"]  = 2\n",
    "standalone_parameters[\"train_k_factor\"] = 1\n",
    "standalone_parameters[\"val_k_factor\"] = 2\n",
    "standalone_parameters[\"test_k_factor\"] = 2\n",
    "\n",
    "\n",
    "standalone_parameters[\"n_epoch\"] = 50\n",
    "\n",
    "standalone_parameters[\"patience\"] = 10\n",
    "standalone_parameters[\"criteria_for_best\"] = \"source_loss\"\n",
    "\n",
    "standalone_parameters[\"datasets\"] = [\n",
    "    {\n",
    "        \"labels\": ALL_SERIAL_NUMBERS,\n",
    "        \"domains\": ALL_DISTANCES_FEET_NARROWED,\n",
    "        \"num_examples_per_domain_per_label\": 100,\n",
    "        \"pickle_path\": os.path.join(get_datasets_base_path(), \"oracle.Run1_framed_2000Examples_stratified_ds.2022A.pkl\"),\n",
    "        \"source_or_target_dataset\": \"source\",\n",
    "        \"x_transforms\": [\"unit_mag\", \"minus_two\"],\n",
    "        \"episode_transforms\": [],\n",
    "        \"domain_prefix\": \"ORACLE_\"\n",
    "    },\n",
    "    {\n",
    "        \"labels\": ALL_NODES,\n",
    "        \"domains\": ALL_DAYS,\n",
    "        \"num_examples_per_domain_per_label\": 100,\n",
    "        \"pickle_path\": os.path.join(get_datasets_base_path(), \"cores.stratified_ds.2022A.pkl\"),\n",
    "        \"source_or_target_dataset\": \"target\",\n",
    "        \"x_transforms\": [\"unit_power\", \"times_zero\"],\n",
    "        \"episode_transforms\": [],\n",
    "        \"domain_prefix\": \"CORES_\"\n",
    "    }   \n",
    "]\n",
    "\n",
    "standalone_parameters[\"torch_default_dtype\"] = \"torch.float32\" \n",
    "\n",
    "\n",
    "\n",
    "standalone_parameters[\"x_net\"] =     [\n",
    "    {\"class\": \"nnReshape\", \"kargs\": {\"shape\":[-1, 1, 2, 256]}},\n",
    "    {\"class\": \"Conv2d\", \"kargs\": { \"in_channels\":1, \"out_channels\":256, \"kernel_size\":(1,7), \"bias\":False, \"padding\":(0,3), },},\n",
    "    {\"class\": \"ReLU\", \"kargs\": {\"inplace\": True}},\n",
    "    {\"class\": \"BatchNorm2d\", \"kargs\": {\"num_features\":256}},\n",
    "\n",
    "    {\"class\": \"Conv2d\", \"kargs\": { \"in_channels\":256, \"out_channels\":80, \"kernel_size\":(2,7), \"bias\":True, \"padding\":(0,3), },},\n",
    "    {\"class\": \"ReLU\", \"kargs\": {\"inplace\": True}},\n",
    "    {\"class\": \"BatchNorm2d\", \"kargs\": {\"num_features\":80}},\n",
    "    {\"class\": \"Flatten\", \"kargs\": {}},\n",
    "\n",
    "    {\"class\": \"Linear\", \"kargs\": {\"in_features\": 80*256, \"out_features\": 256}}, # 80 units per IQ pair\n",
    "    {\"class\": \"ReLU\", \"kargs\": {\"inplace\": True}},\n",
    "    {\"class\": \"BatchNorm1d\", \"kargs\": {\"num_features\":256}},\n",
    "\n",
    "    {\"class\": \"Linear\", \"kargs\": {\"in_features\": 256, \"out_features\": 256}},\n",
    "]\n",
    "\n",
    "# Parameters relevant to results\n",
    "# These parameters will basically never need to change\n",
    "standalone_parameters[\"NUM_LOGS_PER_EPOCH\"] = 10\n",
    "standalone_parameters[\"BEST_MODEL_PATH\"] = \"./best_model.pth\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fa1cc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters not injected, running with standalone_parameters\n",
      "FUCK\n",
      "<class 'tuple'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Set this to True if you want to run this template directly\n",
    "STANDALONE = False\n",
    "if STANDALONE:\n",
    "    print(\"parameters not injected, running with standalone_parameters\")\n",
    "    parameters = standalone_parameters\n",
    "\n",
    "if not 'parameters' in locals() and not 'parameters' in globals():\n",
    "    raise Exception(\"Parameter injection failed\")\n",
    "\n",
    "#Use an easy dict for all the parameters\n",
    "p = EasyDict(parameters)\n",
    "\n",
    "if \"x_shape\" not in p:\n",
    "    p.x_shape = [2,256] # Default to this if we dont supply x_shape\n",
    "\n",
    "\n",
    "supplied_keys = set(p.keys())\n",
    "\n",
    "if  supplied_keys != required_parameters:\n",
    "    print(\"Parameters are incorrect\")\n",
    "    if len(supplied_keys - required_parameters)>0: print(\"Shouldn't have:\", str(supplied_keys - required_parameters))\n",
    "    if len(required_parameters - supplied_keys)>0: print(\"Need to have:\", str(required_parameters - supplied_keys))\n",
    "    raise RuntimeError(\"Parameters are incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a028d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Set the RNGs and make it all deterministic\n",
    "###################################\n",
    "np.random.seed(p.seed)\n",
    "random.seed(p.seed)\n",
    "torch.manual_seed(p.seed)\n",
    "\n",
    "torch.use_deterministic_algorithms(True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b691acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# The stratified datasets honor this\n",
    "###########################################\n",
    "torch.set_default_dtype(eval(p.torch_default_dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5fba671",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Build the network(s)\n",
    "# Note: It's critical to do this AFTER setting the RNG\n",
    "###################################\n",
    "x_net = build_sequential(p.x_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d7e61cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_secs = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6b67dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.domains_source = []\n",
    "p.domains_target = []\n",
    "\n",
    "\n",
    "train_original_source = []\n",
    "val_original_source   = []\n",
    "test_original_source  = []\n",
    "\n",
    "train_original_target = []\n",
    "val_original_target   = []\n",
    "test_original_target  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3d9c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_x_transform_func = lambda x: normalize(x.to(torch.get_default_dtype()), \"unit_power\") # unit_power, unit_mag\n",
    "# global_x_transform_func = lambda x: normalize(x, \"unit_power\") # unit_power, unit_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acb78fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dataset(\n",
    "    labels,\n",
    "    domains,\n",
    "    pickle_path,\n",
    "    x_transforms,\n",
    "    episode_transforms,\n",
    "    domain_prefix,\n",
    "    num_examples_per_domain_per_label,\n",
    "    source_or_target_dataset:str,\n",
    "    iterator_seed=p.seed,\n",
    "    dataset_seed=p.dataset_seed,\n",
    "    n_shot=p.n_shot,\n",
    "    n_way=p.n_way,\n",
    "    n_query=p.n_query,\n",
    "    train_val_test_k_factors=(p.train_k_factor,p.val_k_factor,p.test_k_factor),\n",
    "):\n",
    "   \n",
    "    if x_transforms == []: x_transform = None\n",
    "    else: x_transform = get_chained_transform(x_transforms)\n",
    "    \n",
    "    if episode_transforms == []: episode_transform = None\n",
    "    else: raise Exception(\"episode_transforms not implemented\")\n",
    "    \n",
    "    episode_transform = lambda tup, _prefix=domain_prefix: (_prefix + str(tup[0]), tup[1])\n",
    "\n",
    "\n",
    "    eaf = Episodic_Accessor_Factory(\n",
    "        labels=labels,\n",
    "        domains=domains,\n",
    "        num_examples_per_domain_per_label=num_examples_per_domain_per_label,\n",
    "        iterator_seed=iterator_seed,\n",
    "        dataset_seed=dataset_seed,\n",
    "        n_shot=n_shot,\n",
    "        n_way=n_way,\n",
    "        n_query=n_query,\n",
    "        train_val_test_k_factors=train_val_test_k_factors,\n",
    "        pickle_path=pickle_path,\n",
    "        x_transform_func=x_transform,\n",
    "    )\n",
    "\n",
    "    train, val, test = eaf.get_train(), eaf.get_val(), eaf.get_test()\n",
    "    train = Lazy_Iterable_Wrapper(train, episode_transform)\n",
    "    val = Lazy_Iterable_Wrapper(val, episode_transform)\n",
    "    test = Lazy_Iterable_Wrapper(test, episode_transform)\n",
    "\n",
    "    if source_or_target_dataset==\"source\":\n",
    "        train_original_source.append(train)\n",
    "        val_original_source.append(val)\n",
    "        test_original_source.append(test)\n",
    "\n",
    "        p.domains_source.extend(\n",
    "            [domain_prefix + str(u) for u in domains]\n",
    "        )\n",
    "    elif source_or_target_dataset==\"target\":\n",
    "        train_original_target.append(train)\n",
    "        val_original_target.append(val)\n",
    "        test_original_target.append(test)\n",
    "        p.domains_target.extend(\n",
    "            [domain_prefix + str(u) for u in domains]\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(f\"invalid source_or_target_dataset: {source_or_target_dataset}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe266617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/wd500GB/CSC500/csc500-main/csc500-utils/steves_utils/transforms.py:42: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return x/np.sqrt(get_average_power(x))\n"
     ]
    }
   ],
   "source": [
    "for ds in p.datasets:\n",
    "    add_dataset(**ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b90d65ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from steves_utils.CORES.utils import (\n",
    "#     ALL_NODES,\n",
    "#     ALL_NODES_MINIMUM_1000_EXAMPLES,\n",
    "#     ALL_DAYS\n",
    "# )\n",
    "\n",
    "# add_dataset(\n",
    "#     labels=ALL_NODES,\n",
    "#     domains = ALL_DAYS,\n",
    "#     num_examples_per_domain_per_label=100,\n",
    "#     pickle_path=os.path.join(get_datasets_base_path(), \"cores.stratified_ds.2022A.pkl\"),\n",
    "#     source_or_target_dataset=\"target\",\n",
    "#     x_transform_func=global_x_transform_func,\n",
    "#     domain_modifier=lambda u: f\"cores_{u}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76db484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from steves_utils.ORACLE.utils_v2 import (\n",
    "#     ALL_DISTANCES_FEET,\n",
    "#     ALL_RUNS,\n",
    "#     ALL_SERIAL_NUMBERS,\n",
    "# )\n",
    "\n",
    "\n",
    "# add_dataset(\n",
    "#     labels=ALL_SERIAL_NUMBERS,\n",
    "#     domains = list(set(ALL_DISTANCES_FEET) - {2,62}),\n",
    "#     num_examples_per_domain_per_label=100,\n",
    "#     pickle_path=os.path.join(get_datasets_base_path(), \"oracle.Run2_framed_2000Examples_stratified_ds.2022A.pkl\"),\n",
    "#     source_or_target_dataset=\"source\",\n",
    "#     x_transform_func=global_x_transform_func,\n",
    "#     domain_modifier=lambda u: f\"oracle1_{u}\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97dea9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from steves_utils.ORACLE.utils_v2 import (\n",
    "#     ALL_DISTANCES_FEET,\n",
    "#     ALL_RUNS,\n",
    "#     ALL_SERIAL_NUMBERS,\n",
    "# )\n",
    "\n",
    "\n",
    "# add_dataset(\n",
    "#     labels=ALL_SERIAL_NUMBERS,\n",
    "#     domains = list(set(ALL_DISTANCES_FEET) - {2,62,56}),\n",
    "#     num_examples_per_domain_per_label=100,\n",
    "#     pickle_path=os.path.join(get_datasets_base_path(), \"oracle.Run2_framed_2000Examples_stratified_ds.2022A.pkl\"),\n",
    "#     source_or_target_dataset=\"source\",\n",
    "#     x_transform_func=global_x_transform_func,\n",
    "#     domain_modifier=lambda u: f\"oracle2_{u}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f289866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_dataset(\n",
    "#     labels=list(range(19)),\n",
    "#     domains = [0,1,2],\n",
    "#     num_examples_per_domain_per_label=100,\n",
    "#     pickle_path=os.path.join(get_datasets_base_path(), \"metehan.stratified_ds.2022A.pkl\"),\n",
    "#     source_or_target_dataset=\"target\",\n",
    "#     x_transform_func=global_x_transform_func,\n",
    "#     domain_modifier=lambda u: f\"met_{u}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86a863c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from steves_utils.wisig.utils import (\n",
    "# #     ALL_NODES_MINIMUM_100_EXAMPLES,\n",
    "# #     ALL_NODES_MINIMUM_500_EXAMPLES,\n",
    "# #     ALL_NODES_MINIMUM_1000_EXAMPLES,\n",
    "# #     ALL_DAYS\n",
    "# # )\n",
    "\n",
    "# import steves_utils.wisig.utils as wisig\n",
    "\n",
    "\n",
    "# add_dataset(\n",
    "#     labels=wisig.ALL_NODES_MINIMUM_100_EXAMPLES,\n",
    "#     domains = wisig.ALL_DAYS,\n",
    "#     num_examples_per_domain_per_label=100,\n",
    "#     pickle_path=os.path.join(get_datasets_base_path(), \"wisig.node3-19.stratified_ds.2022A.pkl\"),\n",
    "#     source_or_target_dataset=\"target\",\n",
    "#     x_transform_func=global_x_transform_func,\n",
    "#     domain_modifier=lambda u: f\"wisig_{u}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd5442bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Build the dataset\n",
    "###################################\n",
    "train_original_source = Iterable_Aggregator(train_original_source, p.seed)\n",
    "val_original_source = Iterable_Aggregator(val_original_source, p.seed)\n",
    "test_original_source = Iterable_Aggregator(test_original_source, p.seed)\n",
    "\n",
    "\n",
    "train_original_target = Iterable_Aggregator(train_original_target, p.seed)\n",
    "val_original_target = Iterable_Aggregator(val_original_target, p.seed)\n",
    "test_original_target = Iterable_Aggregator(test_original_target, p.seed)\n",
    "\n",
    "# For CNN We only use X and Y. And we only train on the source.\n",
    "# Properly form the data using a transform lambda and Lazy_Iterable_Wrapper. Finally wrap them in a dataloader\n",
    "\n",
    "transform_lambda = lambda ex: ex[1] # Original is (<domain>, <episode>) so we strip down to episode only\n",
    "\n",
    "train_processed_source = Lazy_Iterable_Wrapper(train_original_source, transform_lambda)\n",
    "val_processed_source   = Lazy_Iterable_Wrapper(val_original_source, transform_lambda)\n",
    "test_processed_source  = Lazy_Iterable_Wrapper(test_original_source, transform_lambda)\n",
    "\n",
    "train_processed_target = Lazy_Iterable_Wrapper(train_original_target, transform_lambda)\n",
    "val_processed_target   = Lazy_Iterable_Wrapper(val_original_target, transform_lambda)\n",
    "test_processed_target  = Lazy_Iterable_Wrapper(test_original_target, transform_lambda)\n",
    "\n",
    "datasets = EasyDict({\n",
    "    \"source\": {\n",
    "        \"original\": {\"train\":train_original_source, \"val\":val_original_source, \"test\":test_original_source},\n",
    "        \"processed\": {\"train\":train_processed_source, \"val\":val_processed_source, \"test\":test_processed_source}\n",
    "    },\n",
    "    \"target\": {\n",
    "        \"original\": {\"train\":train_original_target, \"val\":val_original_target, \"test\":test_original_target},\n",
    "        \"processed\": {\"train\":train_processed_target, \"val\":val_processed_target, \"test\":test_processed_target}\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b3c01fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ORACLE_20', 'ORACLE_26', 'ORACLE_32', 'ORACLE_8', 'ORACLE_38', 'ORACLE_44', 'ORACLE_14', 'ORACLE_50'}\n",
      "{'CORES_4', 'CORES_5', 'CORES_1', 'CORES_2', 'CORES_3'}\n",
      "tensor([[[-0.7070, -0.7054, -0.7072,  ..., -0.7086, -0.7081, -0.7092],\n",
      "         [-0.7080, -0.7083, -0.7055,  ..., -0.7061, -0.7091, -0.7071]],\n",
      "\n",
      "        [[-0.7078, -0.7086, -0.7068,  ..., -0.7055, -0.7057, -0.7050],\n",
      "         [-0.7055, -0.7068, -0.7092,  ..., -0.7071, -0.7053, -0.7087]],\n",
      "\n",
      "        [[-0.7072, -0.7092, -0.7062,  ..., -0.7052, -0.7081, -0.7067],\n",
      "         [-0.7064, -0.7078, -0.7082,  ..., -0.7068, -0.7051, -0.7051]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.7079, -0.7070, -0.7061,  ..., -0.7063, -0.7088, -0.7086],\n",
      "         [-0.7068, -0.7091, -0.7068,  ..., -0.7054, -0.7068, -0.7070]],\n",
      "\n",
      "        [[-0.7068, -0.7055, -0.7082,  ..., -0.7084, -0.7077, -0.7086],\n",
      "         [-0.7080, -0.7072, -0.7059,  ..., -0.7067, -0.7091, -0.7072]],\n",
      "\n",
      "        [[-0.7069, -0.7054, -0.7075,  ..., -0.7084, -0.7083, -0.7085],\n",
      "         [-0.7080, -0.7078, -0.7057,  ..., -0.7061, -0.7089, -0.7071]]])\n"
     ]
    }
   ],
   "source": [
    "from steves_utils.transforms import get_average_magnitude, get_average_power\n",
    "\n",
    "print(set([u for u,_ in val_original_source]))\n",
    "print(set([u for u,_ in val_original_target]))\n",
    "\n",
    "s_x, s_y, q_x, q_y, _ = next(iter(train_processed_source))\n",
    "print(s_x)\n",
    "\n",
    "# for ds in [\n",
    "#     train_processed_source,\n",
    "#     val_processed_source,\n",
    "#     test_processed_source,\n",
    "#     train_processed_target,\n",
    "#     val_processed_target,\n",
    "#     test_processed_target\n",
    "# ]:\n",
    "#     for s_x, s_y, q_x, q_y, _ in ds:\n",
    "#         for X in (s_x, q_x):\n",
    "#             for x in X:\n",
    "#                 assert np.isclose(get_average_magnitude(x.numpy()), 1.0)\n",
    "#                 assert np.isclose(get_average_power(x.numpy()), 1.0)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbdacba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 256)\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# Build the model\n",
    "###################################\n",
    "# easfsl only wants a tuple for the shape\n",
    "model = Steves_Prototypical_Network(x_net, device=p.device, x_shape=tuple(p.x_shape))\n",
    "optimizer = Adam(params=model.parameters(), lr=p.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b39ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# train\n",
    "###################################\n",
    "jig = PTN_Train_Eval_Test_Jig(model, p.BEST_MODEL_PATH, p.device)\n",
    "\n",
    "jig.train(\n",
    "    train_iterable=datasets.source.processed.train,\n",
    "    source_val_iterable=datasets.source.processed.val,\n",
    "    target_val_iterable=datasets.target.processed.val,\n",
    "    num_epochs=p.n_epoch,\n",
    "    num_logs_per_epoch=p.NUM_LOGS_PER_EPOCH,\n",
    "    patience=p.patience,\n",
    "    optimizer=optimizer,\n",
    "    criteria_for_best=p.criteria_for_best,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e8fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_experiment_time_secs = time.time() - start_time_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b9595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Evaluate the model\n",
    "###################################\n",
    "source_test_label_accuracy, source_test_label_loss = jig.test(datasets.source.processed.test)\n",
    "target_test_label_accuracy, target_test_label_loss = jig.test(datasets.target.processed.test)\n",
    "\n",
    "source_val_label_accuracy, source_val_label_loss = jig.test(datasets.source.processed.val)\n",
    "target_val_label_accuracy, target_val_label_loss = jig.test(datasets.target.processed.val)\n",
    "\n",
    "history = jig.get_history()\n",
    "\n",
    "total_epochs_trained = len(history[\"epoch_indices\"])\n",
    "\n",
    "val_dl = Iterable_Aggregator((datasets.source.original.val,datasets.target.original.val))\n",
    "\n",
    "confusion = ptn_confusion_by_domain_over_dataloader(model, p.device, val_dl)\n",
    "per_domain_accuracy = per_domain_accuracy_from_confusion(confusion)\n",
    "\n",
    "# Add a key to per_domain_accuracy for if it was a source domain\n",
    "for domain, accuracy in per_domain_accuracy.items():\n",
    "    per_domain_accuracy[domain] = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"source?\": domain in p.domains_source\n",
    "    }\n",
    "\n",
    "# Do an independent accuracy assesment JUST TO BE SURE!\n",
    "# _source_test_label_accuracy = independent_accuracy_assesment(model, datasets.source.processed.test, p.device)\n",
    "# _target_test_label_accuracy = independent_accuracy_assesment(model, datasets.target.processed.test, p.device)\n",
    "# _source_val_label_accuracy = independent_accuracy_assesment(model, datasets.source.processed.val, p.device)\n",
    "# _target_val_label_accuracy = independent_accuracy_assesment(model, datasets.target.processed.val, p.device)\n",
    "\n",
    "# assert(_source_test_label_accuracy == source_test_label_accuracy)\n",
    "# assert(_target_test_label_accuracy == target_test_label_accuracy)\n",
    "# assert(_source_val_label_accuracy == source_val_label_accuracy)\n",
    "# assert(_target_val_label_accuracy == target_val_label_accuracy)\n",
    "\n",
    "experiment = {\n",
    "    \"experiment_name\": p.experiment_name,\n",
    "    \"parameters\": dict(p),\n",
    "    \"results\": {\n",
    "        \"source_test_label_accuracy\": source_test_label_accuracy,\n",
    "        \"source_test_label_loss\": source_test_label_loss,\n",
    "        \"target_test_label_accuracy\": target_test_label_accuracy,\n",
    "        \"target_test_label_loss\": target_test_label_loss,\n",
    "        \"source_val_label_accuracy\": source_val_label_accuracy,\n",
    "        \"source_val_label_loss\": source_val_label_loss,\n",
    "        \"target_val_label_accuracy\": target_val_label_accuracy,\n",
    "        \"target_val_label_loss\": target_val_label_loss,\n",
    "        \"total_epochs_trained\": total_epochs_trained,\n",
    "        \"total_experiment_time_secs\": total_experiment_time_secs,\n",
    "        \"confusion\": confusion,\n",
    "        \"per_domain_accuracy\": per_domain_accuracy,\n",
    "    },\n",
    "    \"history\": history,\n",
    "    \"dataset_metrics\": get_dataset_metrics(datasets, \"ptn\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a21829",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = get_loss_curve(experiment)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a0f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results_table(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8dc689",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_domain_accuracies(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ae082",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Source Test Label Accuracy:\", experiment[\"results\"][\"source_test_label_accuracy\"], \"Target Test Label Accuracy:\", experiment[\"results\"][\"target_test_label_accuracy\"])\n",
    "print(\"Source Val Label Accuracy:\", experiment[\"results\"][\"source_val_label_accuracy\"], \"Target Val Label Accuracy:\", experiment[\"results\"][\"target_val_label_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacca602",
   "metadata": {
    "tags": [
     "experiment_json"
    ]
   },
   "outputs": [],
   "source": [
    "json.dumps(experiment)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
