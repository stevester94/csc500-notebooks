{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fea5f46",
   "metadata": {},
   "source": [
    "# Transfer Learning Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0902182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "    \n",
    "import os, json, sys, time, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from  easydict import EasyDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from steves_models.steves_ptn import Steves_Prototypical_Network\n",
    "\n",
    "from steves_utils.lazy_iterable_wrapper import Lazy_Iterable_Wrapper\n",
    "from steves_utils.iterable_aggregator import Iterable_Aggregator\n",
    "from steves_utils.ptn_train_eval_test_jig import  PTN_Train_Eval_Test_Jig\n",
    "from steves_utils.torch_sequential_builder import build_sequential\n",
    "from steves_utils.torch_utils import get_dataset_metrics, ptn_confusion_by_domain_over_dataloader\n",
    "from steves_utils.utils_v2 import (per_domain_accuracy_from_confusion, get_datasets_base_path)\n",
    "from steves_utils.PTN.utils import independent_accuracy_assesment\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from steves_utils.stratified_dataset.episodic_accessor import Episodic_Accessor_Factory\n",
    "\n",
    "from steves_utils.ptn_do_report import (\n",
    "    get_loss_curve,\n",
    "    get_results_table,\n",
    "    get_parameters_table,\n",
    "    get_domain_accuracies,\n",
    ")\n",
    "\n",
    "from steves_utils.transforms import get_chained_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c840b4",
   "metadata": {},
   "source": [
    "# Allowed Parameters\n",
    "These are allowed parameters, not defaults\n",
    "Each of these values need to be present in the injected parameters (the notebook will raise an exception if they are not present)\n",
    "\n",
    "Papermill uses the cell tag \"parameters\" to inject the real parameters below this cell.\n",
    "Enable tags to see what I mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd44eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_parameters = {\n",
    "    \"experiment_name\",\n",
    "    \"lr\",\n",
    "    \"device\",\n",
    "    \"seed\",\n",
    "    \"dataset_seed\",\n",
    "    \"n_shot\",\n",
    "    \"n_query\",\n",
    "    \"n_way\",\n",
    "    \"train_k_factor\",\n",
    "    \"val_k_factor\",\n",
    "    \"test_k_factor\",\n",
    "    \"n_epoch\",\n",
    "    \"patience\",\n",
    "    \"criteria_for_best\",\n",
    "    \"x_net\",\n",
    "    \"datasets\",\n",
    "    \"torch_default_dtype\",\n",
    "    \"NUM_LOGS_PER_EPOCH\",\n",
    "    \"BEST_MODEL_PATH\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa3f0049",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "from steves_utils.CORES.utils import (\n",
    "    ALL_NODES,\n",
    "    ALL_NODES_MINIMUM_1000_EXAMPLES,\n",
    "    ALL_DAYS\n",
    ")\n",
    "\n",
    "from steves_utils.ORACLE.utils_v2 import (\n",
    "    ALL_DISTANCES_FEET_NARROWED,\n",
    "    ALL_RUNS,\n",
    "    ALL_SERIAL_NUMBERS,\n",
    ")\n",
    "\n",
    "import steves_utils.wisig.utils as wisig\n",
    "\n",
    "\n",
    "standalone_parameters = {}\n",
    "standalone_parameters[\"experiment_name\"] = \"STANDALONE PTN\"\n",
    "standalone_parameters[\"lr\"] = 0.00001\n",
    "standalone_parameters[\"device\"] = \"cuda\"\n",
    "\n",
    "standalone_parameters[\"seed\"] = 1337\n",
    "standalone_parameters[\"dataset_seed\"] = 1337\n",
    "\n",
    "standalone_parameters[\"n_way\"] = 16\n",
    "standalone_parameters[\"n_shot\"] = 3\n",
    "standalone_parameters[\"n_query\"]  = 2\n",
    "standalone_parameters[\"train_k_factor\"] = 1\n",
    "standalone_parameters[\"val_k_factor\"] = 2\n",
    "standalone_parameters[\"test_k_factor\"] = 2\n",
    "\n",
    "\n",
    "standalone_parameters[\"n_epoch\"] = 100\n",
    "\n",
    "standalone_parameters[\"patience\"] = 10\n",
    "standalone_parameters[\"criteria_for_best\"] = \"target_accuracy\"\n",
    "\n",
    "standalone_parameters[\"datasets\"] = [\n",
    "#     {\n",
    "#         \"labels\": list(set(ALL_NODES)-set(ALL_NODES_MINIMUM_1000_EXAMPLES)),\n",
    "#         \"domains\": ALL_DAYS,\n",
    "#         \"num_examples_per_domain_per_label\": 100,\n",
    "#         \"pickle_path\": os.path.join(get_datasets_base_path(), \"cores.stratified_ds.2022A.pkl\"),\n",
    "#         \"source_or_target_dataset\": \"source\", # Fill in later\n",
    "#         \"x_transforms\": [\"lowpass_+/-10MHz\", \"take_200\"],\n",
    "#         \"episode_transforms\": [],\n",
    "#         \"domain_prefix\": \"C_A_\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"labels\": ALL_NODES_MINIMUM_1000_EXAMPLES,\n",
    "#         \"domains\": ALL_DAYS,\n",
    "#         \"num_examples_per_domain_per_label\": 1000,\n",
    "#         \"pickle_path\": os.path.join(get_datasets_base_path(), \"cores.stratified_ds.2022A.pkl\"),\n",
    "#         \"source_or_target_dataset\": \"source\", # Fill in later\n",
    "#         \"x_transforms\": [\"lowpass_+/-10MHz\", \"take_200\"],\n",
    "#         \"episode_transforms\": [],\n",
    "#         \"domain_prefix\": \"C_B_\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"labels\": list(set(wisig.ALL_NODES_MINIMUM_100_EXAMPLES) - set(wisig.ALL_NODES_MINIMUM_500_EXAMPLES + wisig.ALL_NODES_MINIMUM_1000_EXAMPLES)),\n",
    "#         \"domains\": wisig.ALL_DAYS,\n",
    "#         \"num_examples_per_domain_per_label\": 100,\n",
    "#         \"pickle_path\": os.path.join(get_datasets_base_path(), \"wisig.node3-19.stratified_ds.2022A.pkl\"),\n",
    "#         \"source_or_target_dataset\": \"source\", # Fill in later\n",
    "#         \"x_transforms\": [\"lowpass_+/-10MHz\", \"take_200\"],\n",
    "#         \"episode_transforms\": [],\n",
    "#         \"domain_prefix\": \"W_A_\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"labels\": list(set(wisig.ALL_NODES_MINIMUM_500_EXAMPLES) - set(wisig.ALL_NODES_MINIMUM_1000_EXAMPLES)),\n",
    "#         \"domains\": wisig.ALL_DAYS,\n",
    "#         \"num_examples_per_domain_per_label\": 500,\n",
    "#         \"pickle_path\": os.path.join(get_datasets_base_path(), \"wisig.node3-19.stratified_ds.2022A.pkl\"),\n",
    "#         \"source_or_target_dataset\": \"source\", # Fill in later\n",
    "#         \"x_transforms\": [\"lowpass_+/-10MHz\", \"take_200\"],\n",
    "#         \"episode_transforms\": [],\n",
    "#         \"domain_prefix\": \"W_B_\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"labels\": wisig.ALL_NODES_MINIMUM_1000_EXAMPLES,\n",
    "#         \"domains\": wisig.ALL_DAYS,\n",
    "#         \"num_examples_per_domain_per_label\": 1000,\n",
    "#         \"pickle_path\": os.path.join(get_datasets_base_path(), \"wisig.node3-19.stratified_ds.2022A.pkl\"),\n",
    "#         \"source_or_target_dataset\": \"source\", # Fill in later\n",
    "#         \"x_transforms\": [\"lowpass_+/-10MHz\", \"take_200\"],\n",
    "#         \"episode_transforms\": [],\n",
    "#         \"domain_prefix\": \"W_C_\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"labels\": ALL_SERIAL_NUMBERS,\n",
    "#         \"domains\": ALL_DISTANCES_FEET_NARROWED[:1],\n",
    "#         \"num_examples_per_domain_per_label\": 500,\n",
    "#         \"pickle_path\": os.path.join(get_datasets_base_path(), \"oracle.Run2_framed_2000Examples_stratified_ds.2022A.pkl\"),\n",
    "#         \"source_or_target_dataset\": \"source\", # Fill in later\n",
    "#         \"x_transforms\": [\"jitter_256_10\", \"take_200\", \"resample_20Msps_to_25Msps\"],\n",
    "#         \"episode_transforms\": [],\n",
    "#         \"domain_prefix\": \"O_2_\"\n",
    "#     },\n",
    "    {\n",
    "        \"labels\": ALL_SERIAL_NUMBERS,\n",
    "        \"domains\": ALL_DISTANCES_FEET_NARROWED,\n",
    "        \"num_examples_per_domain_per_label\": 2000,\n",
    "        \"pickle_path\": os.path.join(get_datasets_base_path(), \"oracle.Run1_framed_2000Examples_stratified_ds.2022A.pkl\"),\n",
    "        \"source_or_target_dataset\": \"target\", # Fill in later\n",
    "        \"x_transforms\": [\"take_200\", \"resample_20Msps_to_25Msps\"],\n",
    "        \"episode_transforms\": [],\n",
    "        \"domain_prefix\": \"O_1_\"\n",
    "    },\n",
    "    {\n",
    "        \"labels\": ALL_NODES,\n",
    "        \"domains\": ALL_DAYS,\n",
    "        \"num_examples_per_domain_per_label\": -1,\n",
    "        \"pickle_path\": os.path.join(get_datasets_base_path(), \"cores.stratified_ds.2022A.pkl\"),\n",
    "        \"source_or_target_dataset\": \"source\", # Fill in later\n",
    "        \"x_transforms\": [\"take_200\"],\n",
    "        \"episode_transforms\": [],\n",
    "        \"domain_prefix\": \"C_A_\"\n",
    "    },\n",
    "    {\n",
    "        \"labels\": wisig.ALL_NODES_MINIMUM_100_EXAMPLES,\n",
    "        \"domains\": wisig.ALL_DAYS,\n",
    "        \"num_examples_per_domain_per_label\": -1,\n",
    "        \"pickle_path\": os.path.join(get_datasets_base_path(), \"wisig.node3-19.stratified_ds.2022A.pkl\"),\n",
    "        \"source_or_target_dataset\": \"source\", # Fill in later\n",
    "        \"x_transforms\": [\"take_200\"],\n",
    "        \"episode_transforms\": [],\n",
    "        \"domain_prefix\": \"W_A_\"\n",
    "    }\n",
    "]\n",
    "\n",
    "standalone_parameters[\"torch_default_dtype\"] = \"torch.float32\" \n",
    "\n",
    "\n",
    "\n",
    "standalone_parameters[\"x_net\"] =     [\n",
    "    {\"class\": \"nnReshape\", \"kargs\": {\"shape\":[-1, 1, 2, 200]}},\n",
    "    {\"class\": \"Conv2d\", \"kargs\": { \"in_channels\":1, \"out_channels\":256, \"kernel_size\":[1,7], \"bias\":False, \"padding\":[0,3], },},\n",
    "    {\"class\": \"ReLU\", \"kargs\": {\"inplace\": True}},\n",
    "    {\"class\": \"BatchNorm2d\", \"kargs\": {\"num_features\":256}},\n",
    "\n",
    "    {\"class\": \"Conv2d\", \"kargs\": { \"in_channels\":256, \"out_channels\":80, \"kernel_size\":[2,7], \"bias\":True, \"padding\":[0,3], },},\n",
    "    {\"class\": \"ReLU\", \"kargs\": {\"inplace\": True}},\n",
    "    {\"class\": \"BatchNorm2d\", \"kargs\": {\"num_features\":80}},\n",
    "    {\"class\": \"Flatten\", \"kargs\": {}},\n",
    "\n",
    "    {\"class\": \"Linear\", \"kargs\": {\"in_features\": 80*200, \"out_features\": 256}}, # 80 units per IQ pair\n",
    "    {\"class\": \"ReLU\", \"kargs\": {\"inplace\": True}},\n",
    "    {\"class\": \"BatchNorm1d\", \"kargs\": {\"num_features\":256}},\n",
    "\n",
    "    {\"class\": \"Linear\", \"kargs\": {\"in_features\": 256, \"out_features\": 256}},\n",
    "]\n",
    "\n",
    "# Parameters relevant to results\n",
    "# These parameters will basically never need to change\n",
    "standalone_parameters[\"NUM_LOGS_PER_EPOCH\"] = 10\n",
    "standalone_parameters[\"BEST_MODEL_PATH\"] = \"./best_model.pth\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fa1cc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters not injected, running with standalone_parameters\n"
     ]
    }
   ],
   "source": [
    "# Set this to True if you want to run this template directly\n",
    "STANDALONE = True\n",
    "if STANDALONE:\n",
    "    print(\"parameters not injected, running with standalone_parameters\")\n",
    "    parameters = standalone_parameters\n",
    "\n",
    "if not 'parameters' in locals() and not 'parameters' in globals():\n",
    "    raise Exception(\"Parameter injection failed\")\n",
    "\n",
    "#Use an easy dict for all the parameters\n",
    "p = EasyDict(parameters)\n",
    "\n",
    "supplied_keys = set(p.keys())\n",
    "\n",
    "if  supplied_keys != required_parameters:\n",
    "    print(\"Parameters are incorrect\")\n",
    "    if len(supplied_keys - required_parameters)>0: print(\"Shouldn't have:\", str(supplied_keys - required_parameters))\n",
    "    if len(required_parameters - supplied_keys)>0: print(\"Need to have:\", str(required_parameters - supplied_keys))\n",
    "    raise RuntimeError(\"Parameters are incorrect\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a028d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Set the RNGs and make it all deterministic\n",
    "###################################\n",
    "np.random.seed(p.seed)\n",
    "random.seed(p.seed)\n",
    "torch.manual_seed(p.seed)\n",
    "\n",
    "torch.use_deterministic_algorithms(True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b691acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# The stratified datasets honor this\n",
    "###########################################\n",
    "torch.set_default_dtype(eval(p.torch_default_dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5fba671",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Build the network(s)\n",
    "# Note: It's critical to do this AFTER setting the RNG\n",
    "###################################\n",
    "x_net = build_sequential(p.x_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d7e61cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_secs = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6b67dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.domains_source = []\n",
    "p.domains_target = []\n",
    "\n",
    "\n",
    "train_original_source = []\n",
    "val_original_source   = []\n",
    "test_original_source  = []\n",
    "\n",
    "train_original_target = []\n",
    "val_original_target   = []\n",
    "test_original_target  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3d9c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_x_transform_func = lambda x: normalize(x.to(torch.get_default_dtype()), \"unit_power\") # unit_power, unit_mag\n",
    "# global_x_transform_func = lambda x: normalize(x, \"unit_power\") # unit_power, unit_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acb78fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dataset(\n",
    "    labels,\n",
    "    domains,\n",
    "    pickle_path,\n",
    "    x_transforms,\n",
    "    episode_transforms,\n",
    "    domain_prefix,\n",
    "    num_examples_per_domain_per_label,\n",
    "    source_or_target_dataset:str,\n",
    "    iterator_seed=p.seed,\n",
    "    dataset_seed=p.dataset_seed,\n",
    "    n_shot=p.n_shot,\n",
    "    n_way=p.n_way,\n",
    "    n_query=p.n_query,\n",
    "    train_val_test_k_factors=(p.train_k_factor,p.val_k_factor,p.test_k_factor),\n",
    "):\n",
    "   \n",
    "    if x_transforms == []: x_transform = None\n",
    "    else: x_transform = get_chained_transform(x_transforms)\n",
    "    \n",
    "    if episode_transforms == []: episode_transform = None\n",
    "    else: raise Exception(\"episode_transforms not implemented\")\n",
    "    \n",
    "    episode_transform = lambda tup, _prefix=domain_prefix: (_prefix + str(tup[0]), tup[1])\n",
    "\n",
    "\n",
    "    eaf = Episodic_Accessor_Factory(\n",
    "        labels=labels,\n",
    "        domains=domains,\n",
    "        num_examples_per_domain_per_label=num_examples_per_domain_per_label,\n",
    "        iterator_seed=iterator_seed,\n",
    "        dataset_seed=dataset_seed,\n",
    "        n_shot=n_shot,\n",
    "        n_way=n_way,\n",
    "        n_query=n_query,\n",
    "        train_val_test_k_factors=train_val_test_k_factors,\n",
    "        pickle_path=pickle_path,\n",
    "        x_transform_func=x_transform,\n",
    "    )\n",
    "\n",
    "    train, val, test = eaf.get_train(), eaf.get_val(), eaf.get_test()\n",
    "    train = Lazy_Iterable_Wrapper(train, episode_transform)\n",
    "    val = Lazy_Iterable_Wrapper(val, episode_transform)\n",
    "    test = Lazy_Iterable_Wrapper(test, episode_transform)\n",
    "\n",
    "    if source_or_target_dataset==\"source\":\n",
    "        train_original_source.append(train)\n",
    "        val_original_source.append(val)\n",
    "        test_original_source.append(test)\n",
    "\n",
    "        p.domains_source.extend(\n",
    "            [domain_prefix + str(u) for u in domains]\n",
    "        )\n",
    "    elif source_or_target_dataset==\"target\":\n",
    "        train_original_target.append(train)\n",
    "        val_original_target.append(val)\n",
    "        test_original_target.append(test)\n",
    "        p.domains_target.extend(\n",
    "            [domain_prefix + str(u) for u in domains]\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(f\"invalid source_or_target_dataset: {source_or_target_dataset}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe266617",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in p.datasets:\n",
    "    add_dataset(**ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd5442bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Build the dataset\n",
    "###################################\n",
    "train_original_source = Iterable_Aggregator(train_original_source, p.seed)\n",
    "val_original_source = Iterable_Aggregator(val_original_source, p.seed)\n",
    "test_original_source = Iterable_Aggregator(test_original_source, p.seed)\n",
    "\n",
    "\n",
    "train_original_target = Iterable_Aggregator(train_original_target, p.seed)\n",
    "val_original_target = Iterable_Aggregator(val_original_target, p.seed)\n",
    "test_original_target = Iterable_Aggregator(test_original_target, p.seed)\n",
    "\n",
    "# For CNN We only use X and Y. And we only train on the source.\n",
    "# Properly form the data using a transform lambda and Lazy_Iterable_Wrapper. Finally wrap them in a dataloader\n",
    "\n",
    "transform_lambda = lambda ex: ex[1] # Original is (<domain>, <episode>) so we strip down to episode only\n",
    "\n",
    "train_processed_source = Lazy_Iterable_Wrapper(train_original_source, transform_lambda)\n",
    "val_processed_source   = Lazy_Iterable_Wrapper(val_original_source, transform_lambda)\n",
    "test_processed_source  = Lazy_Iterable_Wrapper(test_original_source, transform_lambda)\n",
    "\n",
    "train_processed_target = Lazy_Iterable_Wrapper(train_original_target, transform_lambda)\n",
    "val_processed_target   = Lazy_Iterable_Wrapper(val_original_target, transform_lambda)\n",
    "test_processed_target  = Lazy_Iterable_Wrapper(test_original_target, transform_lambda)\n",
    "\n",
    "datasets = EasyDict({\n",
    "    \"source\": {\n",
    "        \"original\": {\"train\":train_original_source, \"val\":val_original_source, \"test\":test_original_source},\n",
    "        \"processed\": {\"train\":train_processed_source, \"val\":val_processed_source, \"test\":test_processed_source}\n",
    "    },\n",
    "    \"target\": {\n",
    "        \"original\": {\"train\":train_original_target, \"val\":val_original_target, \"test\":test_original_target},\n",
    "        \"processed\": {\"train\":train_processed_target, \"val\":val_processed_target, \"test\":test_processed_target}\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b3c01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from steves_utils.transforms import get_average_magnitude, get_average_power\n",
    "\n",
    "# print(set([u for u,_ in val_original_source]))\n",
    "# print(set([u for u,_ in val_original_target]))\n",
    "\n",
    "# s_x, s_y, q_x, q_y, _ = next(iter(train_processed_source))\n",
    "# print(s_x)\n",
    "# print(s_x.shape)\n",
    "\n",
    "# for ds in [\n",
    "#     train_processed_source,\n",
    "#     val_processed_source,\n",
    "#     test_processed_source,\n",
    "#     train_processed_target,\n",
    "#     val_processed_target,\n",
    "#     test_processed_target\n",
    "# ]:\n",
    "#     for s_x, s_y, q_x, q_y, _ in ds:\n",
    "#         for X in (s_x, q_x):\n",
    "#             for x in X:\n",
    "#                 assert np.isclose(get_average_magnitude(x.numpy()), 1.0)\n",
    "#                 assert np.isclose(get_average_power(x.numpy()), 1.0)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbdacba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 200)\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# Build the model\n",
    "###################################\n",
    "model = Steves_Prototypical_Network(x_net, device=p.device, x_shape=(2,200))\n",
    "optimizer = Adam(params=model.parameters(), lr=p.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22b39ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, [batch: 1 / 4384], examples_per_second: 67.3165, train_label_loss: 2.2798, \n",
      "epoch: 1, [batch: 439 / 4384], examples_per_second: 3768.6815, train_label_loss: 0.6214, \n",
      "epoch: 1, [batch: 877 / 4384], examples_per_second: 3788.6114, train_label_loss: 0.1957, \n",
      "epoch: 1, [batch: 1315 / 4384], examples_per_second: 3756.5799, train_label_loss: 0.6419, \n",
      "epoch: 1, [batch: 1754 / 4384], examples_per_second: 3765.3332, train_label_loss: 0.1148, \n",
      "epoch: 1, [batch: 2192 / 4384], examples_per_second: 3761.8543, train_label_loss: 0.1719, \n",
      "epoch: 1, [batch: 2630 / 4384], examples_per_second: 3765.8487, train_label_loss: 0.8651, \n",
      "epoch: 1, [batch: 3069 / 4384], examples_per_second: 3768.2432, train_label_loss: 0.0384, \n",
      "epoch: 1, [batch: 3507 / 4384], examples_per_second: 3782.0855, train_label_loss: 0.3866, \n",
      "epoch: 1, [batch: 3945 / 4384], examples_per_second: 3768.2943, train_label_loss: 0.0215, \n",
      "=============================================================\n",
      "epoch: 1, source_val_acc_label: 0.9626, target_val_acc_label: 0.4475, source_val_label_loss: 0.2502, target_val_label_loss: 2.5702, \n",
      "=============================================================\n",
      "New best\n",
      "epoch: 2, [batch: 1 / 4387], examples_per_second: 5.2336, train_label_loss: 0.2047, \n",
      "epoch: 2, [batch: 439 / 4387], examples_per_second: 3757.7431, train_label_loss: 0.0802, \n",
      "epoch: 2, [batch: 878 / 4387], examples_per_second: 3810.7302, train_label_loss: 0.1981, \n",
      "epoch: 2, [batch: 1316 / 4387], examples_per_second: 3786.5637, train_label_loss: 0.0530, \n",
      "epoch: 2, [batch: 1755 / 4387], examples_per_second: 3797.2959, train_label_loss: 0.0389, \n",
      "epoch: 2, [batch: 2194 / 4387], examples_per_second: 3796.3875, train_label_loss: 0.2975, \n",
      "epoch: 2, [batch: 2632 / 4387], examples_per_second: 3807.2367, train_label_loss: 0.3688, \n",
      "epoch: 2, [batch: 3071 / 4387], examples_per_second: 3803.1886, train_label_loss: 0.0200, \n",
      "epoch: 2, [batch: 3509 / 4387], examples_per_second: 3778.5083, train_label_loss: 0.0548, \n",
      "epoch: 2, [batch: 3948 / 4387], examples_per_second: 3762.8805, train_label_loss: 0.0968, \n",
      "=============================================================\n",
      "epoch: 2, source_val_acc_label: 0.9690, target_val_acc_label: 0.4879, source_val_label_loss: 0.1595, target_val_label_loss: 2.3529, \n",
      "=============================================================\n",
      "New best\n",
      "epoch: 3, [batch: 1 / 4388], examples_per_second: 5.2572, train_label_loss: 0.5818, \n",
      "epoch: 3, [batch: 439 / 4388], examples_per_second: 3742.0036, train_label_loss: 0.0485, \n",
      "epoch: 3, [batch: 878 / 4388], examples_per_second: 3786.9083, train_label_loss: 0.1735, \n",
      "epoch: 3, [batch: 1317 / 4388], examples_per_second: 3751.3232, train_label_loss: 0.0584, \n",
      "epoch: 3, [batch: 1755 / 4388], examples_per_second: 3738.2580, train_label_loss: 0.0062, \n",
      "epoch: 3, [batch: 2194 / 4388], examples_per_second: 3742.3701, train_label_loss: 0.0576, \n",
      "epoch: 3, [batch: 2633 / 4388], examples_per_second: 3792.4070, train_label_loss: 0.0154, \n",
      "epoch: 3, [batch: 3071 / 4388], examples_per_second: 3805.9102, train_label_loss: 0.1451, \n",
      "epoch: 3, [batch: 3510 / 4388], examples_per_second: 3787.4305, train_label_loss: 0.0139, \n",
      "epoch: 3, [batch: 3949 / 4388], examples_per_second: 3779.7981, train_label_loss: 0.2057, \n",
      "=============================================================\n",
      "epoch: 3, source_val_acc_label: 0.9754, target_val_acc_label: 0.4953, source_val_label_loss: 0.1200, target_val_label_loss: 2.3646, \n",
      "=============================================================\n",
      "New best\n",
      "epoch: 4, [batch: 1 / 4379], examples_per_second: 5.2489, train_label_loss: 0.0323, \n",
      "epoch: 4, [batch: 438 / 4379], examples_per_second: 3723.9343, train_label_loss: 0.0115, \n",
      "epoch: 4, [batch: 876 / 4379], examples_per_second: 3767.8108, train_label_loss: 0.0351, \n",
      "epoch: 4, [batch: 1314 / 4379], examples_per_second: 3775.2118, train_label_loss: 0.0245, \n",
      "epoch: 4, [batch: 1752 / 4379], examples_per_second: 3739.0729, train_label_loss: 0.0075, \n",
      "epoch: 4, [batch: 2190 / 4379], examples_per_second: 3740.5612, train_label_loss: 0.0103, \n",
      "epoch: 4, [batch: 2627 / 4379], examples_per_second: 3764.1005, train_label_loss: 0.0027, \n",
      "epoch: 4, [batch: 3065 / 4379], examples_per_second: 3773.2053, train_label_loss: 0.0044, \n",
      "epoch: 4, [batch: 3503 / 4379], examples_per_second: 3750.4045, train_label_loss: 0.0027, \n",
      "epoch: 4, [batch: 3941 / 4379], examples_per_second: 3770.0144, train_label_loss: 0.1388, \n",
      "=============================================================\n",
      "epoch: 4, source_val_acc_label: 0.9785, target_val_acc_label: 0.5137, source_val_label_loss: 0.1057, target_val_label_loss: 2.3545, \n",
      "=============================================================\n",
      "New best\n",
      "epoch: 5, [batch: 1 / 4387], examples_per_second: 5.2846, train_label_loss: 0.4988, \n",
      "epoch: 5, [batch: 439 / 4387], examples_per_second: 3719.9221, train_label_loss: 0.0408, \n",
      "epoch: 5, [batch: 878 / 4387], examples_per_second: 3717.1668, train_label_loss: 0.0052, \n",
      "epoch: 5, [batch: 1316 / 4387], examples_per_second: 3773.8397, train_label_loss: 0.0794, \n",
      "epoch: 5, [batch: 1755 / 4387], examples_per_second: 3767.5614, train_label_loss: 0.0015, \n",
      "epoch: 5, [batch: 2194 / 4387], examples_per_second: 3752.7250, train_label_loss: 0.0094, \n",
      "epoch: 5, [batch: 2632 / 4387], examples_per_second: 3767.0086, train_label_loss: 0.3725, \n",
      "epoch: 5, [batch: 3071 / 4387], examples_per_second: 3705.6580, train_label_loss: 0.0983, \n",
      "epoch: 5, [batch: 3509 / 4387], examples_per_second: 3655.5817, train_label_loss: 0.0227, \n",
      "epoch: 5, [batch: 3948 / 4387], examples_per_second: 3800.8332, train_label_loss: 0.0217, \n",
      "=============================================================\n",
      "epoch: 5, source_val_acc_label: 0.9812, target_val_acc_label: 0.5174, source_val_label_loss: 0.0881, target_val_label_loss: 2.2923, \n",
      "=============================================================\n",
      "New best\n",
      "epoch: 6, [batch: 1 / 4387], examples_per_second: 5.2742, train_label_loss: 0.3150, \n",
      "epoch: 6, [batch: 439 / 4387], examples_per_second: 3735.2932, train_label_loss: 0.2124, \n",
      "epoch: 6, [batch: 878 / 4387], examples_per_second: 3757.5222, train_label_loss: 0.0592, \n",
      "epoch: 6, [batch: 1316 / 4387], examples_per_second: 3744.1735, train_label_loss: 0.0336, \n",
      "epoch: 6, [batch: 1755 / 4387], examples_per_second: 3736.6900, train_label_loss: 0.0108, \n",
      "epoch: 6, [batch: 2194 / 4387], examples_per_second: 3696.4734, train_label_loss: 0.0024, \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_157030/2813601670.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mjig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPTN_Train_Eval_Test_Jig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBEST_MODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m jig.train(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msource_val_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/wd500GB/CSC500/csc500-main/csc500-utils/steves_utils/ptn_train_eval_test_jig.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_iterable, source_val_iterable, target_val_iterable, num_epochs, num_logs_per_epoch, patience, optimizer, criteria_for_best)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mnum_examples_processed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mquery_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mbatch_label_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mtrain_label_loss_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_label_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/wd500GB/CSC500/csc500-main/csc500-utils/easyfsl/methods/abstract_meta_learner.py\u001b[0m in \u001b[0;36mfit_on_task\u001b[0;34m(self, support_images, support_labels, query_images, query_labels, optimizer)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/wd500GB/CSC500/csc500-main/venv/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/wd500GB/CSC500/csc500-main/venv/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# train\n",
    "###################################\n",
    "jig = PTN_Train_Eval_Test_Jig(model, p.BEST_MODEL_PATH, p.device)\n",
    "\n",
    "jig.train(\n",
    "    train_iterable=datasets.source.processed.train,\n",
    "    source_val_iterable=datasets.source.processed.val,\n",
    "    target_val_iterable=datasets.target.processed.val,\n",
    "    num_epochs=p.n_epoch,\n",
    "    num_logs_per_epoch=p.NUM_LOGS_PER_EPOCH,\n",
    "    patience=p.patience,\n",
    "    optimizer=optimizer,\n",
    "    criteria_for_best=p.criteria_for_best,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e8fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_experiment_time_secs = time.time() - start_time_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b9595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Evaluate the model\n",
    "###################################\n",
    "source_test_label_accuracy, source_test_label_loss = jig.test(datasets.source.processed.test)\n",
    "target_test_label_accuracy, target_test_label_loss = jig.test(datasets.target.processed.test)\n",
    "\n",
    "source_val_label_accuracy, source_val_label_loss = jig.test(datasets.source.processed.val)\n",
    "target_val_label_accuracy, target_val_label_loss = jig.test(datasets.target.processed.val)\n",
    "\n",
    "history = jig.get_history()\n",
    "\n",
    "total_epochs_trained = len(history[\"epoch_indices\"])\n",
    "\n",
    "val_dl = Iterable_Aggregator((datasets.source.original.val,datasets.target.original.val))\n",
    "\n",
    "confusion = ptn_confusion_by_domain_over_dataloader(model, p.device, val_dl)\n",
    "per_domain_accuracy = per_domain_accuracy_from_confusion(confusion)\n",
    "\n",
    "# Add a key to per_domain_accuracy for if it was a source domain\n",
    "for domain, accuracy in per_domain_accuracy.items():\n",
    "    per_domain_accuracy[domain] = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"source?\": domain in p.domains_source\n",
    "    }\n",
    "\n",
    "# Do an independent accuracy assesment JUST TO BE SURE!\n",
    "# _source_test_label_accuracy = independent_accuracy_assesment(model, datasets.source.processed.test, p.device)\n",
    "# _target_test_label_accuracy = independent_accuracy_assesment(model, datasets.target.processed.test, p.device)\n",
    "# _source_val_label_accuracy = independent_accuracy_assesment(model, datasets.source.processed.val, p.device)\n",
    "# _target_val_label_accuracy = independent_accuracy_assesment(model, datasets.target.processed.val, p.device)\n",
    "\n",
    "# assert(_source_test_label_accuracy == source_test_label_accuracy)\n",
    "# assert(_target_test_label_accuracy == target_test_label_accuracy)\n",
    "# assert(_source_val_label_accuracy == source_val_label_accuracy)\n",
    "# assert(_target_val_label_accuracy == target_val_label_accuracy)\n",
    "\n",
    "experiment = {\n",
    "    \"experiment_name\": p.experiment_name,\n",
    "    \"parameters\": dict(p),\n",
    "    \"results\": {\n",
    "        \"source_test_label_accuracy\": source_test_label_accuracy,\n",
    "        \"source_test_label_loss\": source_test_label_loss,\n",
    "        \"target_test_label_accuracy\": target_test_label_accuracy,\n",
    "        \"target_test_label_loss\": target_test_label_loss,\n",
    "        \"source_val_label_accuracy\": source_val_label_accuracy,\n",
    "        \"source_val_label_loss\": source_val_label_loss,\n",
    "        \"target_val_label_accuracy\": target_val_label_accuracy,\n",
    "        \"target_val_label_loss\": target_val_label_loss,\n",
    "        \"total_epochs_trained\": total_epochs_trained,\n",
    "        \"total_experiment_time_secs\": total_experiment_time_secs,\n",
    "        \"confusion\": confusion,\n",
    "        \"per_domain_accuracy\": per_domain_accuracy,\n",
    "    },\n",
    "    \"history\": history,\n",
    "    \"dataset_metrics\": get_dataset_metrics(datasets, \"ptn\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a21829",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = get_loss_curve(experiment)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a0f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results_table(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8dc689",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_domain_accuracies(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ae082",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Source Test Label Accuracy:\", experiment[\"results\"][\"source_test_label_accuracy\"], \"Target Test Label Accuracy:\", experiment[\"results\"][\"target_test_label_accuracy\"])\n",
    "print(\"Source Val Label Accuracy:\", experiment[\"results\"][\"source_val_label_accuracy\"], \"Target Val Label Accuracy:\", experiment[\"results\"][\"target_val_label_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacca602",
   "metadata": {
    "tags": [
     "experiment_json"
    ]
   },
   "outputs": [],
   "source": [
    "json.dumps(experiment)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
